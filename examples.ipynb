{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd1723a-b3fc-4578-886c-23ddc47df378",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# PyCUDA primal-dual algorithms for TV/TGV-constrained imaging problems â€” Examples\n",
    "This notebook's purpose is to reproduce the numerical experiments in the article:\n",
    "> Kristian Bredies. Recovering piecewise smooth multichannel images by minimization of convex functionals with total generalized variation penalty. _Lecture Notes in Computer Science_, 8293:44-77, 2014. doi:[10.1007/978-3-642-54774-4_3](https://doi.org/10.1007/978-3-642-54774-4_3)\n",
    "\n",
    "In particular, it demonstrates the implemented primal-dual algorithms for solving variational problems with total variation (TV) as well as total generalized variation (TGV) regularization, i.e., problems of the type\n",
    "$$ \\min_u \\ D(u) + R(u) $$\n",
    "where $D$ is a discrepancy functional associated to an imaging problem and $R$ is either the (discrete) total variation\n",
    "$$\n",
    "R(u) = \\alpha_1 \\mathrm{TV}(u) = \\alpha_1 \\| \\nabla u \\|_{1}\n",
    "$$\n",
    "for $\\alpha_0 > 0$ or the (discrete) total generalized variation of second order\n",
    "$$\n",
    "R(u) = \\mathrm{TGV}_\\alpha^2(u) = \\min_w \\ \\alpha_1 \\| \\nabla u - w \\|_1 + \\alpha_0 \\| \\mathcal{E} w \\|_1\n",
    "$$\n",
    "for $\\alpha_0 > 0$, $\\alpha_1 > 0$ and $\\mathcal{E}w = \\frac12(\\nabla u + \\nabla u^\\perp)$ the symmetrized derivative of the vector field $w$. The computations are performed on regular rectangular grids in two dimensions and with multichannel data such as color images. Please confer the above-referenced publication for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a86684-a63f-4b7a-84d2-b6dca8047475",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Initialization\n",
    "First, some necessary modules and functions are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddcbd-8431-4aba-b3cc-e85345e91cb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "from matplotlib.pyplot import subplots, imread, imshow, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebbf18-44ab-463b-b975-b916ad7e60ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Denoising examples\n",
    "The following example code calls the implemented primal-dual algorithms for image denoising with TV/TGV regularization. This method aims at solving the optimization problem\n",
    "$$\n",
    "\\min_u \\ \\frac1p \\| u - f \\|_p^p + R(u) \\ ,\n",
    "$$\n",
    "for $f$ a noisy image and $p \\in \\{1, 2\\}$. The corresponding functions are `tv_denoise` and `tgv_denoise` from `denoise_pycuda`. They are called as follows:\n",
    "```\n",
    "u = tv_denoise(f, alpha1, norm, maxiter, vis)\n",
    "u = tgv_denoise(f, alpha1, fac, norm, maxiter, vis)\n",
    "```\n",
    "Here, `f` is the noisy image, `alpha1` corresponds to the parameter $\\alpha_1$, `fac` determines $\\alpha_0 = \\alpha_1 \\cdot \\mathrm{fac}$, `norm` corresponds to $p$, `maxiter` is the number of iterations and `vis` triggers a visualization of the current iterate every `vis`th iteration if positive. Both functions return a denoised image `u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db970e2-9951-4c53-b7d7-bbef0965a443",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from denoise_pycuda import tv_denoise, tgv_denoise\n",
    "from numpy import random, clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ff72b-dc8c-42f5-9ae7-e3c6e161f061",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Example 1: Face ($L^2$-denoising)\n",
    "This example demonstrates the case $p=2$. Gaussian noise with variance $\\sigma = 0.2$ is added and the denoising methods are called with $\\alpha_1$ that give best PSNR values within a reasonable number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efc15a-2d9b-467b-a121-3a027e4ecfab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(14031621)\n",
    "u_clean = imread(\"test_data/alinas_eye.png\")\n",
    "f = u_clean + random.randn(*u_clean.shape) * 0.2\n",
    "u_tv = tv_denoise(f, 0.305, 2, 500, -1)\n",
    "u_tgv = tgv_denoise(f, 0.288, 2.0, 2, 500, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c51d97-4edf-40c2-8b36-e8c3096c8c2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The results can be visualized via `matplotlib.pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd4dae-440e-4918-83af-b1da35759982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(13, 10))\n",
    "ax1.imshow(u_clean); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(f, 0, 1)); ax2.axis('off'); ax2.set_title('noisy image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV denoising')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV denoising')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d848ca1-4bf4-4107-8129-428c19c48e17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Example 2: Leaf ($L^2$-denoising)\n",
    "This example reproduces Fig. 2 in *LNCS 8293:44-77, 2014*. Again, $p=2$ and $\\alpha_1$ are chosen such that the results are PSNR-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf1a5a-2e18-4459-973e-aaa9d1437b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation\n",
    "random.seed(14031621)\n",
    "u_clean = imread(\"test_data/wet_leaf.png\")\n",
    "f = u_clean + random.randn(*u_clean.shape) * 0.2\n",
    "u_tv = tv_denoise(f, 0.31, 2, 500, -1)\n",
    "u_tgv = tgv_denoise(f, 0.298, 2.0, 2, 500, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(13, 10))\n",
    "ax1.imshow(u_clean); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(f, 0, 1)); ax2.axis('off'); ax2.set_title('noisy image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV denoising')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV denoising')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69c224-4d08-4407-a1f2-4eec37688d4a",
   "metadata": {},
   "source": [
    "### Example 3: Balloons ($L^1$-denoising)\n",
    "This example demonstrates the case $p=1$ and reproduces Fig. 3 in *LNCS 8293:44-77, 2014*. Impulsive noise is applied (1/3 of the data is replaced by random values) and the denoising methods are called with PSNR-optimal $\\alpha_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050d208-77db-4244-bd96-3e76f2912f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation\n",
    "random.seed(14031621)\n",
    "u_clean = imread(\"test_data/balloons2.png\")\n",
    "noise = random.rand(*u_clean.shape)\n",
    "pattern = random.rand(*u_clean.shape) < 1/3\n",
    "f = u_clean * (1 - pattern) + noise * pattern\n",
    "u_tv = tv_denoise(f, 0.86, 1, 1000, -1)\n",
    "u_tgv = tgv_denoise(f, 0.82, 2.0, 1, 1000, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(10, 10))\n",
    "ax1.imshow(u_clean); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(f, 0, 1)); ax2.axis('off'); ax2.set_title('noisy image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV denoising')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV denoising')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f52fdd-1cd5-4b95-9503-66c721ec53be",
   "metadata": {},
   "source": [
    "---\n",
    "## Deblurring examples\n",
    "The implemented primal-dual algorithms aim at deblurring a blurry image by solving a regularized deconvolution problem via the Tikhonov functional minimization \n",
    "$$\n",
    "\\min_u \\ \\frac12 \\| u \\ast k - f \\|_2^2 + R(u) \\ ,\n",
    "$$\n",
    "where $f$ is the blurry, noisy image and $k$ is the blurring kernel. The corresponding functions are `tv_deblur` and `tgv_deblur` from `deblur_pycuda`. They are called as follows:\n",
    "```\n",
    "u = tv_deblur(f, k, alpha1, maxiter, vis)\n",
    "u = tgv_deblur(f, k, alpha1, fac, maxiter, vis)\n",
    "```\n",
    "Here, `f` is the blurry, noisy image, `k` represents the blurring kernel, `alpha1` corresponds to the parameter $\\alpha_1$, `fac` determines $\\alpha_0 = \\alpha_1 \\cdot \\mathrm{fac}$, `maxiter` is the number of iterations and `vis` triggers a visualization of the current iterate every `vis`th iteration if positive. Both functions return a deblurred image `u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0070ffd-eb24-4228-9f96-6bbe3e278b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deblur_pycuda import tv_deblur, tgv_deblur\n",
    "from linop_pycuda import ConvolutionOperator\n",
    "from pycuda import gpuarray\n",
    "from numpy import meshgrid, sum, random, clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc70017-0863-4b65-8b3f-e99207f71a95",
   "metadata": {},
   "source": [
    "### Example 1: Face ($L^2$-deblurring)\n",
    "This example reproduces Fig. 4 in *LNCS 8293:44-77, 2014*. The original image is convolved with an out-of-focus kernel with a diameter of 15 pixels and Gaussian noise with variance $\\sigma = 0.05$ is added. As before, the $\\alpha_1$ are chosen such that the results are PSNR-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6217ea-3a15-4cc3-92a4-eaea044239a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurring kernel and operator\n",
    "x = range(-7, 8)\n",
    "[X, Y] = meshgrid(x, x)\n",
    "mask = (X * X + Y * Y <= 7 * 7).astype('float32')\n",
    "mask = mask / sum(mask[:])\n",
    "K = ConvolutionOperator(mask)\n",
    "\n",
    "# data generation\n",
    "random.seed(14031621)\n",
    "u_clean = imread(\"test_data/alinas_eye512.png\")\n",
    "u_gpu = gpuarray.to_gpu(u_clean.astype('float32').copy(order='F'))\n",
    "Ku = gpuarray.zeros(K.get_dest_shape(u_gpu.shape), 'float32', order='F')\n",
    "K.apply(u_gpu, Ku)\n",
    "f = Ku.get() + random.randn(*Ku.shape) * 0.05\n",
    "\n",
    "# computation\n",
    "u_tv = tv_deblur(f, mask, 0.0101, 1000, -1)\n",
    "u_tgv = tgv_deblur(f, mask, 0.0085, 4.0, 1000, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(13, 10))\n",
    "ax1.imshow(u_clean); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(f, 0, 1)); ax2.axis('off'); ax2.set_title('blurred, noisy image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV deblurring')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV deblurring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64428f-b625-4d02-a3f5-9a26fae39840",
   "metadata": {},
   "source": [
    "### Example 2: Peppers ($L^2$-deblurring)\n",
    "Another deblurring example. Again, an out-of-focus kernel (diameter: 15 pixels) and additive Gaussian noise ($\\sigma = 0.05$) are employed. The $\\alpha_1$ are chosen yield PSNR-optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5c006-8709-46d5-8ff1-532bdd381e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurring kernel and operator\n",
    "x = range(-7, 8)\n",
    "[X, Y] = meshgrid(x, x)\n",
    "mask = (X * X + Y * Y <= 7 * 7).astype('float32')\n",
    "mask = mask / sum(mask[:])\n",
    "K = ConvolutionOperator(mask)\n",
    "\n",
    "# data generation\n",
    "random.seed(14031621)\n",
    "u_clean = imread(\"test_data/peppers.png\")\n",
    "u_gpu = gpuarray.to_gpu(u_clean.astype('float32').copy(order='F'))\n",
    "Ku = gpuarray.zeros(K.get_dest_shape(u_gpu.shape), 'float32', order='F')\n",
    "K.apply(u_gpu, Ku)\n",
    "f = Ku.get() + random.randn(*Ku.shape) * 0.05\n",
    "\n",
    "# computation\n",
    "u_tv = tv_deblur(f, mask, 0.0082, 1000, -1)\n",
    "u_tgv = tgv_deblur(f, mask, 0.0074, 4.0, 1000, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(10, 10))\n",
    "ax1.imshow(u_clean); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(f, 0, 1)); ax2.axis('off'); ax2.set_title('blurred, noisy image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV deblurring')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV deblurring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1982a-ab4f-4697-8d0e-0fa2a1d43829",
   "metadata": {},
   "source": [
    "---\n",
    "## Zooming examples\n",
    "The example code includes primal-dual algorithms for solving zooming problems in the following variational form:\n",
    "$$\n",
    "\\min_u \\ R(u) \\qquad \\text{subject to} \\ \\qquad (Ku)_{i,j} = g_{i,j} \\quad \\forall i=\\{0, \\ldots, N-1\\}, \\ j= \\{0,\\ldots, M-1\\} \\ ,\n",
    "$$\n",
    "where $K$ is a downsampling operator mapping high-resolution images to images of size $N \\times M$ and $(g_{i,j})_{i,j}$ are the respective coefficients of a low-resolution image $f$ of size $N \\times M$. The high-resolution image $u$ could be of any size greater than $N \\times M$. The algorithms implement two downsampling strategies: block averaging and DCT (Discrete Cosine Transform).\n",
    "\n",
    "Zooming with block averaging as downsampling is available via `tv_zoom` and `tgv_zoom` from `zoom_pycuda`:\n",
    "```\n",
    "u = tv_zoom(f, power, alpha1, maxiter, vis)\n",
    "u = tgv_zoom(f, power, alpha1, fac, maxiter, vis)\n",
    "```\n",
    "Zooming with DCT as downsampling is available via `tv_zoom_dct` and `tgv_zoom_dct` from `zoom_pycuda`:\n",
    "```\n",
    "u = tv_zoom_dct(f, power, alpha1, maxiter, vis)\n",
    "u = tgv_zoom_dct(f, power, alpha1, fac, maxiter, vis)\n",
    "```\n",
    "Here, `f` is the low-resolution image (in case of DCT-zooming restricted to the size $2^n \\times 2^m$ for some positive integers $n$ and $m$), the positive integer `power` indicates a zooming factor of $2^{\\mathrm{power}}$, `alpha1` corresponds to the parameter $\\alpha_1$, `fac` determines $\\alpha_0 = \\alpha_1 \\cdot \\mathrm{fac}$, `maxiter` is the number of iterations and `vis` triggers a visualization of the current iterate every `vis`th iteration if positive. Each function returns a high-resolution image `u` with size $2^{\\mathrm{power}}$ times the size of `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c29739-83ea-4c65-b73a-2de91ffc7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoom_pycuda import tv_zoom, tgv_zoom, tv_zoom_dct, tgv_zoom_dct\n",
    "from linop_pycuda import ZoomingOperator, DCTZoomingOperator\n",
    "from common_pycuda import enlarge_next_power_of_2\n",
    "from pycuda import gpuarray\n",
    "from numpy import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cbaf3-0cef-485e-ac00-14356023f131",
   "metadata": {},
   "source": [
    "### Example 1: Violin (Block averaging zooming)\n",
    "An image of size 64 x 64 is zoomed by the factor 8 using averaging over 8 x 8 blocks as downsampling model. The parameters $\\alpha_1$ are chosen to give best PSNR after 4000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c553f1-0420-4de2-9292-5d51fc780cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and downsample example image\n",
    "f = imread(\"test_data/violin.png\")\n",
    "f_gpu = gpuarray.to_gpu(f.astype('float32').copy(order='F'))\n",
    "K = ZoomingOperator(3)\n",
    "u = gpuarray.zeros(K.get_dest_shape(f.shape), 'float32', order='F')\n",
    "K.apply(f_gpu, u)\n",
    "g = u.get()\n",
    "\n",
    "# computation\n",
    "u_tv = tv_zoom(g, 3, 0.13, 4000, -1)\n",
    "u_tgv = tgv_zoom(g, 3, 0.03, 3.0, 4000, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(10, 10))\n",
    "ax1.imshow(f); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(g, 0, 1)); ax2.axis('off'); ax2.set_title('downsampled image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV zooming')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV zooming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460c75f-dc09-4fc6-8e99-5b7242ca126e",
   "metadata": {},
   "source": [
    "### Example 2: Hand (DCT-zooming)\n",
    "This example reproduces Fig. 5 in *LNCS 8293:44-77, 2014*. Again, an image of size 64 x 64 is zoomed by the factor 8. The downsampling model extracts the DCT coefficients associated with the lowest frequencies. The parameters $\\alpha_1$ are chosen to give best PSNR after 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13662e51-e7fb-4159-b602-fce24df17bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and downsample example image\n",
    "f = enlarge_next_power_of_2(imread(\"test_data/hand.png\"))\n",
    "f_gpu = gpuarray.to_gpu(f.astype('float32').copy(order='F'))\n",
    "K = DCTZoomingOperator(3, (f.shape[0] >> 3, f.shape[1] >> 3))\n",
    "u = gpuarray.zeros(K.get_dest_shape(f.shape), 'float32', order='F')\n",
    "K.apply(f_gpu, u)\n",
    "g = u.get() / 8\n",
    "\n",
    "# computation\n",
    "u_tv = tv_zoom_dct(g, 3, 0.019, 2000, -1)\n",
    "u_tgv = tgv_zoom_dct(g, 3, 0.028, 3.8, 2000, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(10, 10))\n",
    "ax1.imshow(f); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(g, 0, 1)); ax2.axis('off'); ax2.set_title('downsampled image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV zooming')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV zooming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cda2ca-43d5-46cf-baf3-612b51448477",
   "metadata": {},
   "source": [
    "---\n",
    "## Dequantization example\n",
    "Dequantization aims at reconstructing an image from quantized image data. A primal-dual algorithm is available in the exmaple code that solves the following variational dequantization approach:\n",
    "$$ \\min_u \\ \\frac1p\\| u - f \\|_2^2 + R(u) \\qquad \\text{subject to} \\qquad f_{\\mathrm{lower}} \\leq u \\leq f_{\\mathrm{upper}} \\ , $$\n",
    "with $f_{\\mathrm{lower}}$ and $f_{\\mathrm{upper}}$ describing the lower and upper bounds of the pointwise quantization intervals, respectively, and $f = \\frac12(f_{\\mathrm{lower}} + f_{\\mathrm{upper}})$. The algorithm can be called via `tv_dequantize`and `tgv_dequantize` in `dequantize_pycuda`:\n",
    "```\n",
    "u = tv_dequantize(lower, upper, alpha1, maxiter, vis)\n",
    "u = tgv_dequantize(lower, upper, alpha1, fac, maxiter, vis)\n",
    "```\n",
    "Here, `lower` and `upper` are the quantization bounds $f_{\\mathrm{lower}}$ and $f_{\\mathrm{upper}}$, respectively, `alpha1` corresponds to the parameter $\\alpha_1$, `fac` determines $\\alpha_0 = \\alpha_1 \\cdot \\mathrm{fac}$, `maxiter` is the number of iterations and `vis` triggers a visualization of the current iterate every `vis`th iteration if positive. Both functions return a dequantized image `u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76759f-16dd-4995-aaaf-39833c48c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dequantize_pycuda import tv_dequantize, tgv_dequantize\n",
    "from pycuda import gpuarray\n",
    "from numpy import clip, floor, ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82543e60-a49b-49aa-892f-026e024aaac3",
   "metadata": {},
   "source": [
    "### Example: Oak leaf ($L^2$-dequantization)\n",
    "This example reproduces Fig. 6 in *LNCS 8293:44-77, 2014*. The original image is quantized with 6 bins per channel. As before, the $\\alpha_1$ are chosen such that the results are PSNR-optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ab827-9b21-4d09-9dc5-65accab7cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and quantize example image\n",
    "f = imread(\"test_data/oak_leaf.png\")\n",
    "lower = floor(f * 6) / 6\n",
    "upper = ceil(f * 6) / 6\n",
    "\n",
    "# computation\n",
    "u_tv = tv_dequantize(lower, upper, 0.49, 2000, -1)\n",
    "u_tgv = tgv_dequantize(lower, upper, 0.45, 2.0, 2000, -1)\n",
    "\n",
    "# visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = subplots(2, 2, figsize=(10, 10))\n",
    "ax1.imshow(f); ax1.axis('off'); ax1.set_title('original image')\n",
    "ax2.imshow(clip(1/2*(lower + upper), 0, 1)); ax2.axis('off'); ax2.set_title('quantized image')\n",
    "ax3.imshow(clip(u_tv, 0, 1)); ax3.axis('off'); ax3.set_title('TV dequantization')\n",
    "ax4.imshow(clip(u_tgv, 0, 1)); ax4.axis('off'); _ = ax4.set_title('TGV dequantization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebbdd9-208d-41c2-9f70-daa8930b9efd",
   "metadata": {},
   "source": [
    "---\n",
    "## Compressive imaging examples\n",
    "Compressive imaging aims at reconstructing an image from a few linear measurements by exploiting sparsity of the data in an appropriate sense. The example code implements a primal-dual algorithm for realizing the following variational compressive imaging model:\n",
    "$$ \\min_u \\ R(u) \\qquad \\text{subject to} \\qquad Ku = f \\ , $$\n",
    "where $K$ maps the image space onto the measurement data $f$. The algorithm for solving such problems are generic Tikhonov functional minimization/linearly constrained minimization solvers: `tv_tikhonov` and `tgv_tikhonov` from `tikhonov_pycuda`:\n",
    "```\n",
    "u = tv_tikhonov(K, f, alpha1, maxiter, vis)\n",
    "u = tgv_tikhonov(K, f, alpha1, fac, maxiter, vis)\n",
    "```\n",
    "Here, $K$ is a general forward operator (an instance of `LinearOperator` from `linop_pycuda`), `f` is the measured data, `alpha1` encodes the regularization parameter and whether Tikhonov functional minimization with $L^2$ discrepancy (`alpha1 > 0` with $\\alpha_1 = $ `alpha1`) or linearly constrained minimization (`alpha1 < 0` and $\\alpha_1 = $ `-alpha1`) is used, `fac` determines $\\alpha_0 = \\alpha_1 \\cdot \\mathrm{fac}$, `maxiter` is the number of iterations and `vis` triggers a visualization of the current iterate every `vis`th iteration if positive. Both functions return an approximation `u` of a solution to the Tikhonov minimization/linearly constrained minimization problem.\n",
    "\n",
    "For compressive imaging, the forward operator $K$ is realized by `AccumulationOperator` from `linop_pycuda`:\n",
    "```\n",
    "K = AccumulationOperator(phi, size)\n",
    "```\n",
    "The parameter `phi` corresponds to the compressive imaging matrix and `size` specifies the size of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f9936-9c30-4024-93de-3fb5f54849c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tikhonov_pycuda import tv_tikhonov, tgv_tikhonov\n",
    "from linop_pycuda import AccumulationOperator\n",
    "from pycuda import gpuarray\n",
    "from numpy import maximum\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ceb022-35ac-44c1-94c1-5c936e418a06",
   "metadata": {},
   "source": [
    "### Example 1: Mug (single-pixel camera data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ff569-6e76-4329-b55f-f3ca828f5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Phi = loadmat(\"compressed_sensing/Phi_64.mat\").get(\"Phi\")\n",
    "f = loadmat(\"compressed_sensing/Mug_64.mat\").get(\"y\")\n",
    "\n",
    "# construct operator\n",
    "K = AccumulationOperator(Phi, (64, 64))\n",
    "\n",
    "# helper function\n",
    "def reconstruct(samples, alpha, maxiter):\n",
    "    u_tv = tv_tikhonov(K, f[0:samples], alpha, maxiter=maxiter, vis=-1)\n",
    "    u_tv = maximum(0.0, u_tv[::-1, :])\n",
    "\n",
    "    u_tgv = tgv_tikhonov(K, f[0:samples], alpha, maxiter=maxiter, vis=-1)\n",
    "    u_tgv = maximum(0.0, u_tgv[::-1, :])\n",
    "\n",
    "    return (samples, u_tv, u_tgv)\n",
    "\n",
    "# reconstruction\n",
    "u = [reconstruct(768, -0.001, 15000), reconstruct(384, -0.001, 20000),\n",
    "     reconstruct(256, -0.001, 30000), reconstruct(192, -0.001, 30000)]\n",
    "\n",
    "# visualization\n",
    "fig, ax = subplots(2, 4, figsize=(13, 6.5))\n",
    "for (i, (samples, u_tv, u_tgv)) in enumerate(u):\n",
    "    ax[0, i].imshow(u_tv, cmap=cm.gray); ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f'TV reconstruction\\n{samples} samples')\n",
    "    ax[1, i].imshow(u_tgv, cmap=cm.gray); ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(f'TGV reconstruction\\n{samples} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f997b0-b57d-49c9-bab9-b51e42afaa61",
   "metadata": {},
   "source": [
    "### Example 2: Violin (synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea13f8f-b0c3-49c1-9978-7fa2a7ad11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Phi = loadmat(\"compressed_sensing/Phi_128.mat\").get(\"Phi\")\n",
    "\n",
    "# construct operator\n",
    "K = AccumulationOperator(Phi, (128, 128))\n",
    "\n",
    "# generate measurement data\n",
    "u = imread('test_data/violin_gray128.png')\n",
    "u_gpu = gpuarray.to_gpu(u[::-1, :].astype('float32').copy(order='F'))\n",
    "f = gpuarray.zeros(K.get_dest_shape(u.shape), 'float32', order='F')\n",
    "K.apply(u_gpu, f); f = f.get()\n",
    "\n",
    "# helper function\n",
    "def reconstruct(samples, alpha, maxiter):\n",
    "    u_tv = tv_tikhonov(K, f[0:samples], alpha, maxiter=maxiter, vis=-1)\n",
    "    u_tv = maximum(0.0, u_tv[::-1, :])\n",
    "\n",
    "    u_tgv = tgv_tikhonov(K, f[0:samples], alpha, maxiter=maxiter, vis=-1)\n",
    "    u_tgv = maximum(0.0, u_tgv[::-1, :])\n",
    "\n",
    "    return (samples, u_tv, u_tgv)\n",
    "\n",
    "# reconstruction\n",
    "u = [reconstruct(3072, -10, 40000), reconstruct(1536, -10, 40000),\n",
    "     reconstruct(1024, -10, 40000), reconstruct(768, -2, 40000)]\n",
    "\n",
    "# visualization\n",
    "fig, ax = subplots(2, 4, figsize=(13, 6.5))\n",
    "for (i, (samples, u_tv, u_tgv)) in enumerate(u):\n",
    "    ax[0, i].imshow(u_tv, cmap=cm.gray); ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f'TV reconstruction\\n{samples} samples')\n",
    "    ax[1, i].imshow(u_tgv, cmap=cm.gray); ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(f'TGV reconstruction\\n{samples} samples')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
